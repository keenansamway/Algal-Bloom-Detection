{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = Path.cwd() / 'data'\n",
    "MODEL_DIR = Path.cwd() / 'models'\n",
    "#IMAGE_DIR = DATA_DIR / 'images'\n",
    "IMAGE_DIR = DATA_DIR / 'sentinel-images'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((480, 480), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.uids = self.df['uid'].tolist()\n",
    "        self.targets = self.df['severity'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_pth = IMAGE_DIR / f\"{self.uids[index]}.png\"\n",
    "        image = Image.open(image_pth)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return self.uids[index], image, self.targets[index] # returns uid, image, target/severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, transform, batch_size=32, shuffle=True, num_workers=0, pin_memory=True):\n",
    "    \n",
    "    dataset = SatelliteDataset(df, transform)\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"EfficientNetV2 Model\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of classes\n",
    "        dropout (float): Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "        # EfficientNetV2 base\n",
    "        # self.pretrained_cnn = models.efficientnet_v2_s(weights='DEFAULT')\n",
    "        # in_features = self.pretrained_cnn.classifier[1].in_features\n",
    "        # self.pretrained_cnn = nn.Sequential(*list(self.pretrained_cnn.children())[:-1])\n",
    "        \n",
    "        \n",
    "        # ResNet50 base\n",
    "        self.pretrained_cnn = models.resnet50(weights='DEFAULT')\n",
    "        in_features = self.pretrained_cnn.fc.in_features\n",
    "        self.pretrained_cnn = nn.Sequential(*list(self.pretrained_cnn.children())[:-1])\n",
    "        \n",
    "        \n",
    "        # Add fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Final softmax layer\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"EfficientNetV2 forward pass\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input image\n",
    "            targets (torch.Tensor): Target labels\n",
    "            \n",
    "        Returns:\n",
    "            preds (torch.Tensor): Predicted labels\n",
    "            loss (torch.Tensor): Loss value\n",
    "        \"\"\"\n",
    "        features = self.pretrained_cnn(x)                 # (batch_size, 1280, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # (batch_size, 1280)\n",
    "        features = self.fc(features)                    # (batch_size, num_classes)\n",
    "        \n",
    "        #preds = self.softmax(features)                  # (batch_size, num_classes)\n",
    "        #preds = torch.argmax(preds, dim=1)              # (batch_size)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            pass\n",
    "        \n",
    "        return features, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(y_features, y_targets):\n",
    "    \"\"\"RMSE Loss\n",
    "    \n",
    "    Args:\n",
    "        y_features (torch.Tensor): Model features\n",
    "        y_targets (torch.Tensor): Target labels\n",
    "        \n",
    "    Returns:\n",
    "        loss (torch.Tensor): Loss value\n",
    "    \"\"\"\n",
    "    y_preds = torch.argmax(F.softmax(y_features, dim=1), dim=1)\n",
    "    \n",
    "    loss = torch.sqrt(F.mse_loss(y_preds, y_targets))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "metadata = pd.read_csv(DATA_DIR / 'sentinel-metadata.csv')\n",
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "\n",
    "train_labels = pd.read_csv(DATA_DIR / 'train_labels.csv')\n",
    "submission_format = pd.read_csv(DATA_DIR / 'submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 5\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entire train set from metadata file\n",
    "train_full = train_labels.merge(\n",
    "    metadata,\n",
    "    how='inner',\n",
    "    left_on='uid',\n",
    "    right_on='uid',\n",
    "    validate='1:1',\n",
    ")\n",
    "train_full = train_full[['uid', 'severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets to better evaluate the model\n",
    "train, validate = train_test_split(train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set from metadata file\n",
    "test_full = metadata[metadata['split'] == 'test']\n",
    "test = submission_format.merge(\n",
    "    test_full,\n",
    "    how='inner',\n",
    "    left_on='uid',\n",
    "    right_on='uid',\n",
    "    validate='1:1',\n",
    ")\n",
    "test = test[['uid', 'region', 'severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "train_loader, train_dataset = get_loader(train, transform, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader, test_dataset = get_loader(validate, transform, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model = CNN(num_classes, dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[5] - Val Loss: 1.0877: 100%|██████████| 226/226 [00:11<00:00, 19.40it/s]  \n",
      "                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     22\u001b[0m \u001b[39m# Update progress bar\u001b[39;00m\n\u001b[1;32m     23\u001b[0m train_images \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/algae/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/algae/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/algae/lib/python3.9/site-packages/torch/optim/adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    160\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    163\u001b[0m           grads,\n\u001b[1;32m    164\u001b[0m           exp_avgs,\n\u001b[1;32m    165\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    167\u001b[0m           state_steps,\n\u001b[1;32m    168\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    169\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    170\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    171\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    176\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/algae/lib/python3.9/site-packages/torch/optim/adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 219\u001b[0m func(params,\n\u001b[1;32m    220\u001b[0m      grads,\n\u001b[1;32m    221\u001b[0m      exp_avgs,\n\u001b[1;32m    222\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    224\u001b[0m      state_steps,\n\u001b[1;32m    225\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    226\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    227\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    228\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    229\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    230\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    232\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/miniconda3/envs/algae/lib/python3.9/site-packages/torch/optim/adamw.py:316\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    314\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    318\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    train_images = 0\n",
    "    train_loss = 0\n",
    "    pbar_train = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    model.train()\n",
    "    for idx, (uid, imgs, targets) in enumerate(pbar_train):\n",
    "        # Move data to device\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass   \n",
    "        outputs, _ = model(imgs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        #rmse = RMSELoss(outputs, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_images += 1\n",
    "        train_loss += loss.item()\n",
    "        desc = (\n",
    "            f\"Epoch [{epoch+1}]/[{num_epochs}]\"\n",
    "            f\" - Train Loss: {train_loss/train_images:.4f}\"\n",
    "        )\n",
    "        pbar_train.set_description(desc=desc)\n",
    "    \n",
    "    # Validation loop    \n",
    "    val_images = 0\n",
    "    val_loss = 0\n",
    "    pbar_val = tqdm(val_loader, total=len(val_loader), leave=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (uid, imgs, targets) in enumerate(pbar_val):\n",
    "            # Move data to device\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, _ = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_images += 1\n",
    "            val_loss += loss.item()\n",
    "            desc = (\n",
    "                f\"Epoch [{epoch+1}]/[{num_epochs}]\"\n",
    "                f\" - Train Loss: {train_loss/train_images:.4f}\"\n",
    "                f\" - Val Loss: {val_loss/val_images:.4f}\"\n",
    "            )\n",
    "            pbar_val.set_description(desc=desc)\n",
    "    \n",
    "    log = (\n",
    "        f\"Epoch [{epoch+1}]/[{num_epochs}]\"\n",
    "        f\" - Train Loss: {train_loss/train_images:.4f}\"\n",
    "        f\" - Val Loss: {val_loss/val_images:.4f}\"\n",
    "    )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pth = MODEL_DIR / 'model.pt'\n",
    "torch.save(model.state_dict(), model_pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(num_classes).to(device)\n",
    "model.load_state_dict(torch.load(model_pth))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5064345b0d58fd2fcf3dda7597a90bdf293cddcb997317e100b9e3dab1db3ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
