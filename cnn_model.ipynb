{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "DATA_DIR = Path.cwd() / 'data'\n",
    "MODEL_DIR = Path.cwd() / 'models'\n",
    "RESULTS_DIR = Path.cwd() / 'results'\n",
    "IMAGE_DIR = DATA_DIR / 'images'\n",
    "# IMAGE_DIR = DATA_DIR / 'sentinel-images'\n",
    "'''\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((480, 480), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((512, 512), interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"EfficientNetV2 Model\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): Number of classes\n",
    "        dropout (float): Dropout rate\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, dropout=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "        # EfficientNetV2 base\n",
    "        # self.pretrained_cnn = models.efficientnet_v2_s(weights='DEFAULT')\n",
    "        # in_features = self.pretrained_cnn.classifier[1].in_features\n",
    "        # self.pretrained_cnn = nn.Sequential(*list(self.pretrained_cnn.children())[:-1])\n",
    "        \n",
    "        \n",
    "        # ResNet50 base\n",
    "        self.pretrained_cnn = models.resnet50(weights='DEFAULT')\n",
    "        in_features = self.pretrained_cnn.fc.in_features\n",
    "        self.pretrained_cnn = nn.Sequential(*list(self.pretrained_cnn.children())[:-1])\n",
    "        \n",
    "        \n",
    "        # Add fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features//2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Final softmax layer\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"EfficientNetV2 forward pass\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input image\n",
    "            \n",
    "        Returns:\n",
    "            preds (torch.Tensor): Predicted labels\n",
    "        \"\"\"\n",
    "        features = self.pretrained_cnn(x)                   # (batch_size, 1280, 1, 1)\n",
    "        features = features.view(features.size(0), -1)      # (batch_size, 1280)\n",
    "        features = self.fc(features)                        # (batch_size, num_classes)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(y_features, y_targets):\n",
    "    \"\"\"RMSE Loss\n",
    "    \n",
    "    Args:\n",
    "        y_features (torch.Tensor): Model features\n",
    "        y_targets (torch.Tensor): Target labels\n",
    "        \n",
    "    Returns:\n",
    "        loss (torch.Tensor): Loss value\n",
    "    \"\"\"\n",
    "    y_preds = torch.argmax(F.softmax(y_features, dim=1), dim=1)\n",
    "    \n",
    "    loss = torch.sqrt(F.mse_loss(y_preds, y_targets))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.uids = self.df['uid'].tolist()\n",
    "        self.targets = self.df['severity'].tolist()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_pth = IMAGE_DIR / f\"{self.uids[index]}.png\"\n",
    "        image = Image.open(image_pth)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return self.uids[index], image, self.targets[index] # returns uid, image, target/severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(df, transform, batch_size=32, shuffle=True, num_workers=0, pin_memory=True):\n",
    "    \n",
    "    dataset = SatelliteDataset(df, transform)\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    \n",
    "    return loader, dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "metadata = pd.read_csv(DATA_DIR / 'saved-metadata.csv')\n",
    "metadata.date = pd.to_datetime(metadata.date)\n",
    "\n",
    "train_labels = pd.read_csv(DATA_DIR / 'train_labels.csv')\n",
    "submission_format = pd.read_csv(DATA_DIR / 'submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_CLASSES = 5\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entire train set from metadata file\n",
    "train_full = train_labels.merge(\n",
    "    metadata,\n",
    "    how='inner',\n",
    "    left_on='uid',\n",
    "    right_on='uid',\n",
    "    validate='1:1',\n",
    ")\n",
    "train_full = train_full[['uid', 'severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets to better evaluate the model\n",
    "train, validate = train_test_split(train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test set from metadata file\n",
    "test_full = metadata[metadata['split'] == 'test']\n",
    "test = submission_format.merge(\n",
    "    test_full,\n",
    "    how='inner',\n",
    "    left_on='uid',\n",
    "    right_on='uid',\n",
    "    validate='1:1',\n",
    ")\n",
    "test = test[['uid', 'region', 'severity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "train_loader, train_dataset = get_loader(train, transform, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader, val_dataset = get_loader(validate, transform, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader, test_dataset = get_loader(test, transform, batch_size=1, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model = CNN(NUM_CLASSES, dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean').to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model_to_save, filename='checkpoint.pt'):\n",
    "    torch.save(model_to_save.state_dict(), MODEL_DIR / filename)\n",
    "\n",
    "best_loss = None\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 1.0151 - Val Loss: 0.9871: 100%|██████████| 226/226 [00:12<00:00, 17.45it/s]\n",
      "Epoch 2/5 - Train Loss: 0.9186 - Val Loss: 0.8820: 100%|██████████| 226/226 [00:12<00:00, 17.80it/s]\n",
      "Epoch 3/5 - Train Loss: 0.8773 - Val Loss: 1.0769: 100%|██████████| 226/226 [00:13<00:00, 16.70it/s]\n",
      "Epoch 4/5 - Train Loss: 0.8860:  11%|█▏        | 103/904 [00:16<01:59,  6.72it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    ## TRAINING LOOP\n",
    "    train_count = 0\n",
    "    train_loss = 0\n",
    "    avg_train_loss = 0\n",
    "    pbar_train = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "    model.train()\n",
    "    for idx, (uids, imgs, targets) in enumerate(pbar_train):\n",
    "        # Move data to device\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass   \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        #rmse = RMSELoss(outputs, targets)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_count += 1\n",
    "        train_loss += loss.item()\n",
    "        avg_train_loss = train_loss/train_count\n",
    "        desc = (\n",
    "            f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "            f\" - Train Loss: {avg_train_loss:.4f}\"\n",
    "        )\n",
    "        pbar_train.set_description(desc=desc)\n",
    "    \n",
    "    \n",
    "    ## VALIDATION LOOP   \n",
    "    val_count = 0\n",
    "    val_loss = 0\n",
    "    avg_val_loss = 0\n",
    "    pbar_val = tqdm(val_loader, total=len(val_loader), leave=True)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (uid, imgs, targets) in enumerate(pbar_val):\n",
    "            # Move data to device\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_count += 1\n",
    "            val_loss += loss.item()\n",
    "            avg_val_loss = val_loss/val_count\n",
    "            desc = (\n",
    "                f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "                f\" - Train Loss: {avg_train_loss:.4f}\"\n",
    "                f\" - Val Loss: {avg_val_loss:.4f}\"\n",
    "            )\n",
    "            pbar_val.set_description(desc=desc)\n",
    "            \n",
    "    \n",
    "    ## CHECKPOINTING AND EARLY STOPPING\n",
    "    if best_loss is None:   # i.e. first epoch\n",
    "        best_loss = avg_val_loss\n",
    "        save_checkpoint(model, filename=f'checkpoint.pt')\n",
    "        \n",
    "    elif avg_val_loss > best_loss:  # i.e. loss increased\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print('Validation loss has not decreased. Stopping training.')\n",
    "            break\n",
    "        \n",
    "    else:   # avg_val_loss < best_loss i.e. loss decreased\n",
    "        best_loss = avg_val_loss\n",
    "        save_checkpoint(model, filename=f'checkpoint.pt')\n",
    "        counter = 0\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(NUM_CLASSES).to(device)\n",
    "\n",
    "model_pth = MODEL_DIR / 'checkpoint.pt'\n",
    "model.load_state_dict(torch.load(model_pth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4275/4275 [01:16<00:00, 55.84it/s]\n"
     ]
    }
   ],
   "source": [
    "submission = submission_format.copy()\n",
    "\n",
    "results = {}\n",
    "\n",
    "model.eval()\n",
    "pbar = tqdm(test_loader, total=len(test_loader), leave=True)\n",
    "for idx, (uid, img, _) in enumerate(pbar):\n",
    "    uid = uid[0]\n",
    "    img = img.to(device)\n",
    "    \n",
    "    output = model(img)\n",
    "    \n",
    "    prediction = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "    \n",
    "    submission.loc[submission.uid == uid, 'severity'] = prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>region</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aabn</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aair</td>\n",
       "      <td>west</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aajw</td>\n",
       "      <td>northeast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aalr</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aalw</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>zzpn</td>\n",
       "      <td>northeast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>zzrv</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>zzsx</td>\n",
       "      <td>south</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>zzvv</td>\n",
       "      <td>west</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6509</th>\n",
       "      <td>zzzi</td>\n",
       "      <td>midwest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6510 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid     region  severity\n",
       "0     aabn       west         4\n",
       "1     aair       west         1\n",
       "2     aajw  northeast         1\n",
       "3     aalr    midwest         1\n",
       "4     aalw       west         4\n",
       "...    ...        ...       ...\n",
       "6505  zzpn  northeast         1\n",
       "6506  zzrv       west         4\n",
       "6507  zzsx      south         1\n",
       "6508  zzvv       west         4\n",
       "6509  zzzi    midwest         1\n",
       "\n",
       "[6510 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4640\n",
       "2       3\n",
       "3     549\n",
       "4    1318\n",
       "Name: severity, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.severity.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4158\n",
       "2     439\n",
       "3     440\n",
       "4    1473\n",
       "Name: severity, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.severity.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(RESULTS_DIR / '2_all-cnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5064345b0d58fd2fcf3dda7597a90bdf293cddcb997317e100b9e3dab1db3ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
